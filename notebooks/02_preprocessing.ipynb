{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378226fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell id=\"prep_001\" language=\"markdown\">\n",
    "# 02 - Data Preprocessing & Feature Engineering\n",
    "\n",
    "This notebook covers comprehensive data preprocessing and feature engineering steps to prepare data for model training.\n",
    "\n",
    "## Steps Covered\n",
    "1. Load and validate data\n",
    "2. Handle missing values\n",
    "3. Detect and treat outliers\n",
    "4. Engineer domain-specific features\n",
    "5. Encode categorical variables\n",
    "6. Scale numerical features\n",
    "7. Prepare train-test split\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_002\" language=\"python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.preprocessor import DataPreprocessor, FeatureEngineer\n",
    "from src.data_loader import DataLoader\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_003\" language=\"python\">\n",
    "# Load and Validate Data\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: DATA LOADING & VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize data loader\n",
    "DATA_PATH = 'data/raw/loan_applications.csv'\n",
    "\n",
    "try:\n",
    "    loader = DataLoader(DATA_PATH)\n",
    "    df = loader.load_data()\n",
    "    print(\"✓ Data loaded and validated successfully\")\n",
    "    \n",
    "    # Display data info\n",
    "    info = loader.get_data_info()\n",
    "    print(\"\\nDataset Information:\")\n",
    "    for key, value in info.items():\n",
    "        if key != 'missing_values' and key != 'features':\n",
    "            print(f\"  {key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    print(\"Creating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'income': np.random.randint(40000, 150000, n_samples),\n",
    "        'loan_amount': np.random.randint(100000, 400000, n_samples),\n",
    "        'credit_score': np.random.randint(600, 850, n_samples),\n",
    "        'employment_years': np.random.randint(0, 30, n_samples),\n",
    "        'age': np.random.randint(25, 65, n_samples),\n",
    "        'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "        'marital_status': np.random.choice(['Single', 'Married', 'Divorced'], n_samples),\n",
    "        'fraud_label': np.random.binomial(1, 0.25, n_samples)\n",
    "    })\n",
    "    print(\"✓ Sample dataset created for demonstration\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_004\" language=\"python\">\n",
    "# Step 2: Handle Missing Values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: MISSING VALUES HANDLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "missing_before = df.isnull().sum().sum()\n",
    "print(f\"\\nMissing values before: {missing_before}\")\n",
    "print(f\"Missing value percentage: {(missing_before / df.size * 100):.2f}%\")\n",
    "\n",
    "# Handle missing values\n",
    "df_filled = DataPreprocessor.handle_missing_values(df, strategy='mean')\n",
    "\n",
    "missing_after = df_filled.isnull().sum().sum()\n",
    "print(f\"\\nMissing values after: {missing_after}\")\n",
    "print(f\"✓ Missing values handled successfully\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_005\" language=\"python\">\n",
    "# Step 3: Outlier Detection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: OUTLIER DETECTION & ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers_iqr(data, feature, iqr_multiplier=1.5):\n",
    "    \"\"\"Detect outliers using Interquartile Range method\"\"\"\n",
    "    Q1 = data[feature].quantile(0.25)\n",
    "    Q3 = data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "    \n",
    "    outliers = data[(data[feature] < lower_bound) | (data[feature] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers for numerical features\n",
    "numerical_features = ['income', 'loan_amount', 'credit_score', 'employment_years', 'age']\n",
    "\n",
    "print(\"\\nOutlier Detection Results (IQR Method):\")\n",
    "for feature in numerical_features:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_filled, feature)\n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Outliers found: {len(outliers)}\")\n",
    "    print(f\"  Bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "    \n",
    "    # Check if outliers are fraud cases\n",
    "    if len(outliers) > 0:\n",
    "        fraud_rate = outliers['fraud_label'].mean() * 100\n",
    "        print(f\"  Fraud rate in outliers: {fraud_rate:.2f}%\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create box plot\n",
    "    box_data = [df_filled[feature]]\n",
    "    bp = ax.boxplot(box_data, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('#3498db')\n",
    "    bp['boxes'][0].set_alpha(0.6)\n",
    "    \n",
    "    # Add scatter plot\n",
    "    y_data = np.random.normal(1, 0.04, size=len(df_filled))\n",
    "    ax.scatter(y_data, df_filled[feature], alpha=0.3, s=30)\n",
    "    \n",
    "    ax.set_ylabel(feature.capitalize(), fontsize=11)\n",
    "    ax.set_title(f'{feature.capitalize()} - Outlier Detection', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.suptitle('Outlier Detection via Box Plots', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Decision: Outliers retained (may represent legitimate extreme cases)\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_006\" language=\"python\">\n",
    "# Step 4: Feature Engineering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply feature engineering\n",
    "engineer = FeatureEngineer()\n",
    "df_engineered = engineer.engineer_features(df_filled)\n",
    "\n",
    "print(\"\\nEngineered Features Created:\")\n",
    "engineered_features = [\n",
    "    'income_to_loan_ratio',\n",
    "    'credit_history_score',\n",
    "    'employment_stability',\n",
    "    'age_credit_interaction',\n",
    "    'loan_amount_category',\n",
    "    'income_category'\n",
    "]\n",
    "\n",
    "for feature in engineered_features:\n",
    "    print(f\"  ✓ {feature}\")\n",
    "    print(f\"    Range: [{df_engineered[feature].min():.4f}, {df_engineered[feature].max():.4f}]\")\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nEngineered Features Statistics:\")\n",
    "print(df_engineered[engineered_features].describe().round(4))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_007\" language=\"python\">\n",
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(engineered_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    legitimate = df_engineered[df_engineered['fraud_label'] == 0][feature]\n",
    "    fraudulent = df_engineered[df_engineered['fraud_label'] == 1][feature]\n",
    "    \n",
    "    ax.hist(legitimate, bins=30, alpha=0.6, label='Legitimate', color='#2ecc71')\n",
    "    ax.hist(fraudulent, bins=30, alpha=0.6, label='Fraudulent', color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Engineered Features Distributions', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature engineering complete\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_008\" language=\"python\">\n",
    "# Step 5: Categorical Encoding\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: CATEGORICAL VARIABLE ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare for encoding\n",
    "df_encoded = df_engineered.copy()\n",
    "\n",
    "categorical_features = ['education_level', 'marital_status']\n",
    "\n",
    "print(\"\\nCategorical Features Encoding:\")\n",
    "print(f\"Original unique values:\")\n",
    "for feature in categorical_features:\n",
    "    print(f\"  {feature}: {df_encoded[feature].unique()}\")\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"\\nAfter one-hot encoding:\")\n",
    "print(f\"  Total features: {df_encoded.shape[1]}\")\n",
    "print(f\"  New columns: {[col for col in df_encoded.columns if col not in df_engineered.columns]}\")\n",
    "\n",
    "# Display encoded data structure\n",
    "print(f\"\\nDataFrame shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"Column names:\\n{df_encoded.columns.tolist()}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_009\" language=\"python\">\n",
    "# Step 6: Feature Scaling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: FEATURE SCALING & STANDARDIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('fraud_label', axis=1)\n",
    "y = df_encoded['fraud_label']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"\\nScaling Results:\")\n",
    "print(f\"Before scaling - Income mean: {X['income'].mean():.2f}, std: {X['income'].std():.2f}\")\n",
    "print(f\"After scaling - Income mean: {X_processed['income'].mean():.4f}, std: {X_processed['income'].std():.4f}\")\n",
    "\n",
    "# Show scaled data statistics\n",
    "print(\"\\nProcessed Features Statistics (After Scaling):\")\n",
    "print(X_processed.describe().round(4))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_010\" language=\"python\">\n",
    "# Visualize scaled vs unscaled\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "features_to_show = ['income', 'loan_amount', 'credit_score', 'employment_years', 'age', 'income_to_loan_ratio']\n",
    "\n",
    "for idx, feature in enumerate(features_to_show):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Unscaled\n",
    "    ax.hist(X[feature], bins=30, alpha=0.5, label='Unscaled', color='#3498db')\n",
    "    \n",
    "    # Scaled (if available)\n",
    "    if feature in X_processed.columns:\n",
    "        ax.hist(X_processed[feature], bins=30, alpha=0.5, label='Scaled', color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} - Before vs After Scaling', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Scaling Impact on Feature Distributions', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Scaling complete\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_011\" language=\"python\">\n",
    "# Step 7: Train-Test Split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain fraud distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain Set Size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"Test Set Size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain Set Fraud Distribution:\")\n",
    "print(f\"  Legitimate: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Fraudulent: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest Set Fraud Distribution:\")\n",
    "print(f\"  Legitimate: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Fraudulent: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Stratified split maintains fraud distribution in both sets\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_012\" language=\"python\">\n",
    "# Visualize train-test split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "datasets = ['Train Set', 'Test Set']\n",
    "fraud_counts = [(y_train == 1).sum(), (y_test == 1).sum()]\n",
    "legitimate_counts = [(y_train == 0).sum(), (y_test == 0).sum()]\n",
    "\n",
    "for idx, (dataset, fraud, legit) in enumerate(zip(datasets, fraud_counts, legitimate_counts)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    categories = ['Legitimate', 'Fraudulent']\n",
    "    counts = [legit, fraud]\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    \n",
    "    bars = ax.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'{dataset} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(count)}\\n({count/(legit+fraud)*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.suptitle('Train-Test Split: Fraud Distribution Maintained', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"prep_013\" language=\"python\">\n",
    "# Summary and Next Steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_info = {\n",
    "    'Original Dataset Size': len(df),\n",
    "    'Final Dataset Size': len(X_processed),\n",
    "    'Original Features': df.shape[1] - 1,  # Exclude target\n",
    "    'Engineered Features': X_processed.shape[1],\n",
    "    'Missing Values Handled': 'Yes',\n",
    "    'Outliers Detected': 'Retained for analysis',\n",
    "    'Categorical Variables Encoded': 'Yes (One-Hot)',\n",
    "    'Numerical Features Scaled': 'Yes (StandardScaler)',\n",
    "    'Train-Test Split Ratio': '80-20',\n",
    "    'Stratification Applied': 'Yes'\n",
    "}\n",
    "\n",
    "for key, value in summary_info.items():\n",
    "    print(f\"{key:.<45} {value}\")\n",
    "\n",
    "print(\"\\n✓ Data preprocessing complete!\")\n",
    "print(\"\\nNext Step: Train multiple models on the preprocessed data\")\n",
    "print(\"See notebook 03_modeling.ipynb for model training and evaluation\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
