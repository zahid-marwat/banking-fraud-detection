{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell id=\"model_001\" language=\"markdown\">\n",
    "# 03 - Model Training & Comparison\n",
    "\n",
    "This notebook covers model training, hyperparameter tuning, and comprehensive evaluation for fraud detection.\n",
    "\n",
    "## Components\n",
    "1. Data preparation and feature scaling\n",
    "2. Model training (Logistic Regression, Random Forest, XGBoost)\n",
    "3. Class imbalance handling with SMOTE\n",
    "4. Model evaluation and comparison\n",
    "5. Hyperparameter tuning\n",
    "6. Ensemble methods\n",
    "7. Feature importance analysis\n",
    "8. Business metrics evaluation\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_002\" language=\"python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.trainer import ModelTrainer\n",
    "from src.evaluator import ModelEvaluator\n",
    "from src.config import ModelConfig\n",
    "from src.preprocessor import FeatureEngineer, DataPreprocessor\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_003\" language=\"python\">\n",
    "# Create sample data for modeling\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: DATA PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 15\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.binomial(1, 0.3, n_samples)  # 30% fraud rate\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset created successfully\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples × {X_train.shape[1]} features\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution (Train):\")\n",
    "print(f\"  Legitimate: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Fraudulent: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution (Test):\")\n",
    "print(f\"  Legitimate: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Fraudulent: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_004\" language=\"python\">\n",
    "# Step 2: Initialize Models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: MODEL INITIALIZATION & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "print(\"\\nTraining Models...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train all models\n",
    "print(\"1. Training Logistic Regression...\")\n",
    "lr_model = trainer.train_logistic_regression(X_train, y_train)\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "print(\"2. Training Random Forest...\")\n",
    "rf_model = trainer.train_random_forest(X_train, y_train)\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "print(\"3. Training XGBoost...\")\n",
    "xgb_model = trainer.train_xgboost(X_train, y_train)\n",
    "print(\"   ✓ Complete\")\n",
    "\n",
    "print(\"\\n✓ All models trained successfully\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_005\" language=\"python\">\n",
    "# Step 3: Make Predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: GENERATING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_dict = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "\n",
    "# Generate predictions for all models\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    probabilities[name] = y_pred_proba\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Predicted labels generated: {len(y_pred)}\")\n",
    "    print(f\"  Fraud predictions: {(y_pred == 1).sum()}\")\n",
    "    print(f\"  Legitimate predictions: {(y_pred == 0).sum()}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_006\" language=\"python\">\n",
    "# Step 4: Model Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"\\nComputing evaluation metrics...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    y_pred_proba = probabilities[name]\n",
    "    \n",
    "    results = evaluator.evaluate(y_test, y_pred, y_pred_proba)\n",
    "    evaluation_results[name] = results\n",
    "    \n",
    "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {results['f1']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_007\" language=\"python\">\n",
    "# Create comprehensive comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': evaluation_results.keys(),\n",
    "    'Accuracy': [results['accuracy'] for results in evaluation_results.values()],\n",
    "    'Precision': [results['precision'] for results in evaluation_results.values()],\n",
    "    'Recall': [results['recall'] for results in evaluation_results.values()],\n",
    "    'F1-Score': [results['f1'] for results in evaluation_results.values()],\n",
    "    'ROC-AUC': [results['roc_auc'] for results in evaluation_results.values()],\n",
    "    'Specificity': [results['specificity'] for results in evaluation_results.values()],\n",
    "    'FPR': [results['false_positive_rate'] for results in evaluation_results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find best model by ROC-AUC\n",
    "best_model_idx = comparison_df['ROC-AUC'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\n✓ Best Model (by ROC-AUC): {best_model_name}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_008\" language=\"python\">\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Specificity']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    values = comparison_df[metric].values\n",
    "    models = comparison_df['Model'].values\n",
    "    \n",
    "    bars = ax.bar(models, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_009\" language=\"python\">\n",
    "# Step 5: Confusion Matrices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: CONFUSION MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Legitimate', 'Fraud'],\n",
    "                yticklabels=['Legitimate', 'Fraud'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Confusion Matrices', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed confusion matrix interpretation\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(\"TN (True Negative):  Legitimate correctly identified\")\n",
    "print(\"FP (False Positive): Legitimate incorrectly marked as fraud (COST)\")\n",
    "print(\"FN (False Negative): Fraud incorrectly accepted as legitimate (RISK)\")\n",
    "print(\"TP (True Positive):  Fraud correctly detected\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_010\" language=\"python\">\n",
    "# Step 6: ROC Curves\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: ROC CURVES COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors_roc = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for (name, y_pred_proba), color in zip(probabilities.items(), colors_roc):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color=color, lw=2.5, \n",
    "            label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Random classifier line\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\", fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ ROC curves generated\")\n",
    "print(\"Note: Higher AUC indicates better discrimination between fraud and legitimate cases\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_011\" language=\"python\">\n",
    "# Step 7: Precision-Recall Curves\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: PRECISION-RECALL CURVES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors_pr = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for (name, y_pred_proba), color in zip(probabilities.items(), colors_pr):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    ax.plot(recall, precision, color=color, lw=2.5,\n",
    "            label=f'{name} (AP = {avg_precision:.3f})')\n",
    "\n",
    "# Baseline\n",
    "baseline_precision = (y_test == 1).sum() / len(y_test)\n",
    "ax.axhline(y=baseline_precision, color='k', linestyle='--', lw=2, \n",
    "           label=f'Baseline (Precision = {baseline_precision:.3f})')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Recall (Fraud Detection Rate)', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower left\", fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Precision-Recall curves generated\")\n",
    "print(\"Note: This curve is more informative for imbalanced datasets than ROC\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_012\" language=\"python\">\n",
    "# Step 8: Feature Importance (Random Forest & XGBoost)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_names = [f'Feature_{i+1}' for i in range(X_train.shape[1])]\n",
    "\n",
    "# Get feature importance for tree-based models\n",
    "rf_importance = rf_model.feature_importances_\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create DataFrames\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_importance\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_importance\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(rf_importance_df.reset_index(drop=True).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (XGBoost):\")\n",
    "print(xgb_importance_df.reset_index(drop=True).to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest\n",
    "ax = axes[0]\n",
    "ax.barh(rf_importance_df['Feature'].iloc[::-1], rf_importance_df['Importance'].iloc[::-1],\n",
    "        color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=11)\n",
    "ax.set_title('Top 10 Features - Random Forest', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "ax = axes[1]\n",
    "ax.barh(xgb_importance_df['Feature'].iloc[::-1], xgb_importance_df['Importance'].iloc[::-1],\n",
    "        color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=11)\n",
    "ax.set_title('Top 10 Features - XGBoost', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Importance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_013\" language=\"python\">\n",
    "# Step 9: Ensemble Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 9: ENSEMBLE MODEL (VOTING CLASSIFIER)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"Ensemble created with:\")\n",
    "print(\"  - Logistic Regression\")\n",
    "print(\"  - Random Forest\")\n",
    "print(\"  - XGBoost\")\n",
    "print(\"  - Voting: Soft (probability averaging)\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "evaluator_ensemble = ModelEvaluator()\n",
    "ensemble_results = evaluator_ensemble.evaluate(y_test, y_pred_ensemble, y_pred_proba_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Model Performance:\")\n",
    "print(f\"  Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {ensemble_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {ensemble_results['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {ensemble_results['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {ensemble_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\n✓ Ensemble performance compared with individual models\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_014\" language=\"python\">\n",
    "# Step 10: Cross-Validation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nCross-validating {name}...\")\n",
    "    \n",
    "    # ROC-AUC scores\n",
    "    roc_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'roc_auc_mean': roc_scores.mean(),\n",
    "        'roc_auc_std': roc_scores.std(),\n",
    "        'f1_mean': f1_scores.mean(),\n",
    "        'f1_std': f1_scores.std(),\n",
    "        'roc_auc_scores': roc_scores,\n",
    "        'f1_scores': f1_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  ROC-AUC: {roc_scores.mean():.4f} (+/- {roc_scores.std():.4f})\")\n",
    "    print(f\"  F1-Score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")\n",
    "\n",
    "# Create summary table\n",
    "cv_summary = pd.DataFrame({\n",
    "    'Model': cv_results.keys(),\n",
    "    'ROC-AUC Mean': [v['roc_auc_mean'] for v in cv_results.values()],\n",
    "    'ROC-AUC Std': [v['roc_auc_std'] for v in cv_results.values()],\n",
    "    'F1 Mean': [v['f1_mean'] for v in cv_results.values()],\n",
    "    'F1 Std': [v['f1_std'] for v in cv_results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + cv_summary.round(4).to_string(index=False))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_015\" language=\"python\">\n",
    "# Visualize cross-validation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models_list = list(cv_results.keys())\n",
    "roc_means = [cv_results[m]['roc_auc_mean'] for m in models_list]\n",
    "roc_stds = [cv_results[m]['roc_auc_std'] for m in models_list]\n",
    "f1_means = [cv_results[m]['f1_mean'] for m in models_list]\n",
    "f1_stds = [cv_results[m]['f1_std'] for m in models_list]\n",
    "\n",
    "# ROC-AUC CV Results\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(models_list))\n",
    "ax.bar(x_pos, roc_means, yerr=roc_stds, capsize=10, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax.set_ylabel('ROC-AUC', fontsize=11)\n",
    "ax.set_title('Cross-Validation: ROC-AUC', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# F1-Score CV Results\n",
    "ax = axes[1]\n",
    "ax.bar(x_pos, f1_means, yerr=f1_stds, capsize=10, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax.set_ylabel('F1-Score', fontsize=11)\n",
    "ax.set_title('Cross-Validation: F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Cross-validation results visualized\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_016\" language=\"python\">\n",
    "# Step 11: Business Metrics & Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 11: BUSINESS METRICS & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nKEY BUSINESS METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, results in evaluation_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  False Positive Rate: {results['false_positive_rate']:.4f}\")\n",
    "    print(f\"    → {results['false_positives']} legitimate apps rejected (COST)\")\n",
    "    print(f\"  Fraud Detection Rate: {results['fraud_detection_rate']:.4f}\")\n",
    "    print(f\"    → {results['true_positives']} fraud cases caught (BENEFIT)\")\n",
    "    print(f\"  Specificity: {results['specificity']:.4f}\")\n",
    "    print(f\"    → Ability to identify legitimate applications\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "1. BEST MODEL FOR PRODUCTION:\n",
    "   - Select model based on business priorities\n",
    "   - Fraud detection (Recall) vs False positives (Specificity) trade-off\n",
    "   - Consider ensemble approach for robustness\n",
    "\n",
    "2. THRESHOLD TUNING:\n",
    "   - Default threshold = 0.5 (50% probability)\n",
    "   - Adjust threshold to balance precision/recall\n",
    "   - Lower threshold → More fraud detection but more false positives\n",
    "   - Higher threshold → Fewer false positives but miss fraud\n",
    "\n",
    "3. HANDLING CLASS IMBALANCE:\n",
    "   ✓ SMOTE applied during training\n",
    "   ✓ Stratified cross-validation used\n",
    "   ✓ Evaluate using appropriate metrics (ROC-AUC, F1, Precision-Recall)\n",
    "   ✗ Avoid accuracy as primary metric\n",
    "\n",
    "4. MODEL DEPLOYMENT:\n",
    "   - Monitor model performance in production\n",
    "   - Retrain periodically as fraud patterns evolve\n",
    "   - Log predictions for auditing and model improvement\n",
    "   - Implement A/B testing for threshold changes\n",
    "\n",
    "5. ENSEMBLE ADVANTAGES:\n",
    "   - Combines strengths of multiple models\n",
    "   - Reduces risk of single model failure\n",
    "   - Often provides better generalization\n",
    "   - Recommended for critical applications\n",
    "\n",
    "6. FEATURE ENGINEERING:\n",
    "   - Top features show good discriminative power\n",
    "   - Consider domain expertise in feature creation\n",
    "   - Monitor feature importance over time\n",
    "   - Remove redundant features if needed\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell id=\"model_017\" language=\"python\">\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODELING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Models Trained',\n",
    "        'Evaluation Metrics',\n",
    "        'Cross-Validation Folds',\n",
    "        'Test Set Size',\n",
    "        'Imbalance Ratio',\n",
    "        'Best Metric Score',\n",
    "        'Feature Count',\n",
    "        'Ensemble Used'\n",
    "    ],\n",
    "    'Value': [\n",
    "        '3 (LR, RF, XGB)',\n",
    "        'Accuracy, Precision, Recall, F1, ROC-AUC',\n",
    "        '5',\n",
    "        f'{len(X_test)} samples',\n",
    "        f'{(y_test==0).sum()}:{(y_test==1).sum()}',\n",
    "        f'{comparison_df[\"ROC-AUC\"].max():.4f}',\n",
    "        X_train.shape[1],\n",
    "        'Yes (Voting Classifier)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + summary_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n✓ Model training and evaluation complete!\")\n",
    "print(\"✓ Ready for deployment or further optimization\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
